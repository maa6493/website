---
title: 'Project 2: Modeling, Testing, and Predicting'
author: "Moses Alfaro (maa6493)"
date: "11/25/2019"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
chandata<-read.csv("chandata.csv")
library(ggplot2)
library(dplyr)
library(sandwich)
library(lmtest)
library(tidyverse)
library(MASS)
library(plotROC)
```

## Introduction

*For my project, I used the website: https://vincentarelbundock.github.io/Rdatasets/datasets.html as my source of finding my dataset for this project. The dataset that I am working with is called "Channing House Data". This dataset contains data on the Channing Retirement House located in Fresno, California and stores collected data on men and women that passed through this house from 1964 until July 1, 1975. Their age on entry and also on leaving or death was recorded along with their sex. A large number of these observations were censored mainly due to the resident being alive on July 1, 1975 when the data was collected. The age at when they died and their gender were also marked down in this dataset. Differences between the survival of the sexes, taking age into account, was one of the primary concerns of the study. Overall the variables in this set are "sex", "entry", "exit", "time", and "cens". The "sex" variable demonstrates whether one is male or female; the "entry" variable represents the age in months at when the person entered into the Channing House; the "exit" variable represents the age in months of the resident on death, leaving the centre or July 1, 1975, whichever event occured first; the "time" variable represents the length of time (in months) that the resident spent at Channing House; the "cens" variable represents the indicator of right censoring (1 indicates that the resident died at channing house and 0 represents that they left the house prior to July 1st, 1975, or that they were still alive in the centre at that date).*

##Part One

1. (15 pts) Perform a MANOVA testing whether any of your numeric variables (or a subset of them, if including them all doesn’t make sense) show a mean difference across levels of one of your categorical variables (3). If they do, perform univariate ANOVAs to find response(s) showing a mean difference across groups (3), and perform post-hoc t tests to find which groups differ (3). Discuss the number of tests you have performed, calculate the probability of at least one type I error (if unadjusted), and adjust the significance level accordingly (bonferroni correction) before discussing significant differences (3). Briefly discuss assumptions and whether or not they are likely to have been met (2).

```{r}
#MANOVA test
sex <- chandata$sex
exit <- chandata$exit
entry <- chandata$entry
time<- chandata$time
res.man <- manova(cbind(exit, entry, time) ~ sex, data = chandata)
summary (res.man)

#Bonferroni correction
0.05/4

#Probability of type-1 error
1-0.95^4


```

*After running the MANOVA test for my numeric variables (exit, entry, and time) against the categorical variable (sex), there was shown to be no signficant value, which indicates no mean difference across levels of my catgeorical variable, sex, against the numeric variables. As Dr. Woodward stated, since my MANOVA was not significant, I do not have to continue with further tests and I just explain what tests I would've done if my results were significant. If my MANOVA test was signficant for these variables, I would of have to of had to perform 4 total tests (including MANOVA). But, MANOVA already tells us what groups differ and we wouldn't of have to perfrom these t-tests to find signficance against the different groups. To calculate the bonferroni correction, we divide the alpha (0.05), which indicates signficance if the p-value is under this value, by the number of tests I would've ran if my MANOVA results would've shown signficance. This bonferroni correction would be 0.0125 which would allow us to remain at a 0.05 error rate to keep the signifcance. The type-one error rate we would get in this case is 0.1854938 based on the number of tests we ran and using this formula (1-0.95^4) which gives us how much error we could probably encounter. The main assumptions for a MANOVA test are 1) random samples, independent observations; 2) multivariate normality of DVs; 3) Homogenity of within-group covariance matrices; 4) linear relationships among DVs; 5) no extreme univariate or multivariate outliers; 6) no multicollinearity. There are a lot of assumptions for this test, and as said in lecture, they are hard to test and meet! We know we meet assumptions like random samples and independent observations, but testing out the other assumptions can be tricky for this MANOVA test and you have to eyeball them off of ggplots for the variables you are observing (sex, entry, exit, time) and look for what assumptions you need. *


## Part Two

2. (10 pts) Perform some kind of randomization test on your data (that makes sense). This can be anything you want! State null and alternative hypotheses, perform the test, and interpret the results (7). Create a plot visualizing the null distribution and the test statistic (3).
```{r}
cens <- chandata$cens
##One-Way ANOVA Test to compare means of exit data against sex categorical variable

#Boxplot of null distributions
ggplot(chandata, aes(x = sex, y = exit)) +
  geom_boxplot(fill = "grey80", colour = "blue") +
  scale_x_discrete() + xlab("Sex") +
  ylab("exit")

#lm test to determine differences of sex and exit while using an ANOVA after lm test
chan_lm = lm(exit ~ sex, data = chandata)
summary(chan_lm)

# Analysis of variance using ANOVA
anova_one_way <- aov(exit~sex, data=chandata)
summary(anova_one_way)

# Confidence Intervals
confint(chan_lm)

# Model Residuals Plot 
chan.mod = data.frame(Fitted = fitted(chan_lm),
  Residuals = resid(chan_lm), Treatment = chandata$sex)

ggplot(chan.mod, aes(Fitted, Residuals, colour = sex)) + geom_point()
```

*The null hypothesis for this ANOVA test is that the mean of the exit time (variable) is the same for both the female and male variables.*

*The alternative hypothesis for this ANOVA test is that one of the means from either the female or male's mean exit time is different from each other.   *

*The results indicate that there are no signficant mean differences between female and males' exit times. We see this from the p-value not being signifcant (0.4022) after running the ANOVA test. We can also visualize these results in the boxplot presented above. We see that visually there is also no difference between the males and females' exit times in months. Lastly, we plotted the residuals against the fitted values to test the assumptions and we see that there is an even spread of residuals for both males and females which is another indicator of there being no signficant difference in mean exit times.*


##Part Three

3. (35 pts) Build a linear regression model predicting one of your response variables from at least 2other variables, including their interaction. Mean-center any numeric variables involved in the interaction.
– Interpret the coefficient estimates (do not discuss significance) (10)
– Plot the regression using ggplot(). If your interaction is numeric by numeric, refer to code near the end of WS15 to make the plot. If you have 3 or more predictors, just chose two to plot for convenience. (7)
– Check assumptions of linearity, normality, and homoskedasticity either graphically or using a hypothesis test (3)
– Regardless, recompute regression results with robust standard errors via coeftest(..., vcov=vcovHC(...)). Discuss significance of results, including any changes from before/after robust SEs if applicable. (7)
– What proportion of the variation in the outcome does your model explain? (3)
```{r}
#Multiple Regressions with and without interactions of the sex/exit variables against the response variable, time. Numeric values not mean-centered for this part, but are in next part. 
fit<-lm(time ~ sex + exit, data=chandata)
fit_int<- lm(time ~ sex*exit, data = chandata)
summary(fit)
summary (fit_int)

#Mean centering of numberic variables 
chandata$time_mc <- chandata$time - mean(chandata$time)
chandata$entry_mc <- chandata$entry - mean(chandata$entry)
chandata$exit_mc <- chandata$exit - mean(chandata$exit)

#Multiple Regression Test with Interactions of sex & exit variable against time response variable 
fit_int_mc <- lm(time_mc ~ sex* exit_mc, data = chandata)
summary(fit_int_mc)

#GGPlot of the Regression 
ggplot(chandata, aes(x=entry_mc, y=time_mc,group=sex))+geom_point(aes(color=sex))+
 geom_smooth(method="lm",formula=y~1,se=F,fullrange=T,aes(color=sex))+
theme(legend.position=c(.9,.19))+xlab("")

# Check for  Linearity and Homoskedasticity 
resids<-fit_int_mc$residuals
fitvals<-fit_int_mc$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')
bptest(fit_int_mc)

# Check for Normality Assumption
ggplot()+geom_histogram(aes(resids), bins=20)

# Before compute with robust standard errors
summary(fit_int_mc)$coef[,1:2]

# Recompute of Regression Results with Robust Standard Errors
library(sandwich)
library(lmtest)
coeftest(fit, vcov = vcovHC(fit_int_mc))[,1:2]

# Proportion of the variation in the outcome explained by the model
(sum((time-mean(time))^2)-sum(fit_int_mc$residuals^2))/sum((time-mean(time))^2)
decimalvalue <- (sum((time-mean(time))^2)-sum(fit_int_mc$residuals^2))/sum((time-mean(time))^2)
decimalvalue*100

# Rerun of regression without interactions (only main effects)
fit<-lm(time ~ sex + exit, data=chandata)
summary(fit)


```

*When looking at the coefficients of this test, we see that when we control for sex, we see that there is a significant of exit time on the time variable that was spent at Channing House for the patients. For every one month increase in the exit time there at channing house, we see the time spent there increase by 0.275 months for each resident of the Channing House. The p-value was less than 0.05 indicating its significance. We also see that the sex of the residents has a significant impact on the time spent in the Channing House (p-value < 0.05). We see that being a male actually reduced the time that they spent in the house by 10.4 months compared to the female residents of Channing house. This could indicate that the males died off faster or exited the house for other reasons than the females.  *

*The significance of the recompute of the regression with robust standard errors shows that the sex still has a signficant effect on the time spent at the house, but we see the standard errors reduced signficantly compared to the original regression model for the intercept. Which indicates that this model is a good predictor of time from the variable sex and not exit time.*

*The proportion of variation that comes from the outcome explained from the model is 18.27% as it was calculated up above in the r chunk,*


##Part Four
4. (5 pts) Rerun same regression model (with interaction), but this time compute bootstrapped standard errors. Discuss any changes you observe in SEs and p-values using these SEs compared to the original SEs and the robust SEs)
```{r}
# Bootstrapped SEs (resampling)
fitted<-fit_int_mc$fitted.values 
resid_resamp<-replicate(5000,{
new_resids<-sample(resids,replace=TRUE) 
chandata$new_y<-fitted+new_resids 
fit_int_mc2<-lm(new_y~sex*exit_mc,data=chandata)
coef(fit_int_mc2) 
})
resid_resamp%>%t%>%as.data.frame%>%summarize_all(sd)

# Original SEs 
coeftest(fit_int_mc)[,1:2]

# Robust SEs
coeftest(fit_int_mc, vcov=vcovHC(fit_int_mc))[,1:2] 
```


*The bootstrapping does not implemenet signficant change to the standard errors or p-values. There seems to be a slight increases in the SEs we see in the boostrapping SEs from the robust and original regressions. This can indicate that this method is prone to more error than the robust or original models that were conducted previously as we this increase in error. *


##Part Five
5. (40 pts) Perform a logistic regression predicting a binary categorical variable (if you don’t have one, make/get one) from at least two explanatory variables (interaction not necessary).
– Interpret coefficient estimates in context (10)
– Report a confusion matrix for your logistic regression (2)
– Compute and discuss the Accuracy, Sensitivity (TPR), Specificity (TNR), and Recall (PPV) of your model (5)
– Using ggplot, plot density of log-odds (logit) by your binary outcome variable (3)
– Generate an ROC curve (plot) and calculate AUC (either manually or with a package); interpret (10)
– Perform 10-fold (or repeated random sub-sampling) CV and report average out-of-sample Accuracy, Sensitivity, and Recall (10)
```{r}
#Logistic Regression
fit2<-glm(cens~ entry + exit, data = chandata, family = binomial (link = "logit"))
coeftest(fit2)

#coefficients
exp(coef(fit2))
log_odds <-exp(coef(fit2))

#Confusion Matrix
prob.cens<- predict(fit2, type = "response")
table(predict=as.numeric(prob.cens>0.5), truth=chandata$cens)%>%addmargins

#Accuracy
(234+53)/462

#Sensitivty 
234/357

#Specificty 
53/105

#Precision
234/286

# ggplot of density of log-odds (logit) by binary outcome variable (cens)



# ROC Curve & AUC calculation
chandata<-chandata%>%mutate(prob=predict(fit2, type="response"), prediction=ifelse(prob>.5,1,0))
newdata<-chandata%>%transmute(prob,prediction,truth=cens)
newdata%>%glimpse()
ROCplot<-ggplot(newdata)+geom_roc(aes(d=truth,m=prob), n.cuts=0) 
ROCplot
calc_auc(ROCplot)

# 10-fold CV and average out-of-sample Accuracy, Sensitivity, and Recall 
class_diag<-function(probs,truth){
  
 tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
 acc=sum(diag(tab))/sum(tab)
 sens=tab[2,2]/colSums(tab)[2]
 spec=tab[1,1]/colSums(tab)[1]
 ppv=tab[2,2]/rowSums(tab)[2]
 if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1

 #Exact AUC calculation
 ord<-order(probs, decreasing=TRUE)
 probs <- probs[ord]; truth <- truth[ord]
 
 TPR=cumsum(truth)/max(1,sum(truth))
 FPR=cumsum(!truth)/max(1,sum(!truth))
 
 dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
 TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
 
 n <- length(TPR)
 auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
 
 data.frame(acc,sens,spec,ppv,auc)
} 

#10-fold CV
set.seed(1234)
k=10
data3<-newdata[sample(nrow(newdata)),]
folds4<-cut(seq(1:nrow(newdata)),breaks=k,labels=F)
diags4<-NULL
for(i in 1:k){
 train<-data3[folds4!=i,]
 test<-data3[folds4==i,]
 truth<-test$prob
 fit<-glm(prob~(.)^2,data=train,family="binomial")
 probs<-predict(fit,newdata = test,type="response")
 diags5<-rbind(diags4,class_diag(probs,truth))
}
apply(diags5,2,mean)

```

*From the coefficients, we see that both the entry and exit variables are a significant predictor of cens, which is the binary variable indicating whether or not a resident has died or left the Channing House.  *

*The accuracy from the confusion matrix of the original linear regression model explains that 62.12% of residents who died in or who left Channing House were accurately measured (cens variable). For sensitivity (true-positive rate), we see that 65.54% of the deaths in channing house were correctly classified. For specificity (true-negative), we see that 50.47% of the residents being classified as leaving Channing House are correctly classified. Lastly, for precision we see that 81.81% is the value, which indicates the classified cases over the correctly classified. This is a high number which indicates that over eigth-tenths of the predictions were correctly classified (precision).  *

*Our AUC was found to be 0.669, which means the 66.9% of the time this model can explain whether cens can be found based on the entry and exit variable. This is a low AUC, and shows that the ROC model is not a well predictor. Another model set/regression model could be found that has a higher AUC to predict the cens off of entry and exit. *

*From the 10-fold CV ran, the results that were given are acc (0.024), sens (0.0), spec (1,0), ppv (0.0), and the auc to be 1.0. We can see that the auc is very high, which is a good indicator that this model of regression can greatly predict cens (binary variable) off of the entry and exit variables since it has such a large area.*

##Part Six
6. (10 pts) Choose one variable you want to predict (can be one you used from before; either binary or continuous) and run a LASSO regression inputting all the rest of your variables as predictors. Choose lambda to give the simplest model whose accuracy is near that of the best (i.e., lambda.1se). Discuss which variables are retained. Perform 10-fold CV using this model: if response in binary, compare model’s out-of-sample accuracy to that of your logistic regression in part 5; if response is numeric, compare the residual standard error (at the bottom of the summary output, aka RMSE): lower is better fit!
```{r}
# LASSO regression Part 1
fit3<-lm(cens~time+exit+entry,data=chandata)
yhat<-predict(fit3)
mean((chandata$cens-yhat)^2) 


#Lasso regression part 2
library(glmnet)
y<-as.matrix(chandata$cens)
x<-chandata%>%dplyr::select(exit, entry, time)%>%mutate_all(scale)%>%as.matrix
cv<-cv.glmnet(x,y)
lasso1<-glmnet(x,y,lambda=cv$lambda.1se)
coef(lasso1)

#10-fold Cross Validation before LASSO regression added in
set.seed(1234)
k=10 
data1<-chandata[sample(nrow(chandata)),] 
folds<-cut(seq(1:nrow(chandata)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
 train<-data1[folds!=i,]
 test<-data1[folds==i,]
 fit4<-lm(cens~time+exit+entry,data=chandata)
 yhat2<-predict(fit4,newdata=test)
 diags<-mean((test$cens-yhat2)^2)
}
mean(diags)



```

*The variables that were retained in the lasso regression were time and entry, while exit was not retained. This indicates that entry and time are the most important predictors of cens (which means if the patients died or left the Channing House). When comparing the 10-fold CV we see that this CV output is smaller which indicates a better fit than the model in part 5.  *